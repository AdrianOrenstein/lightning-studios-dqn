import os
import torch
from typing import Callable, Optional
import time
from lightning.fabric import Fabric
from tqdm import tqdm
import wandb
from wandb.integration.lightning.fabric import WandbLogger
from dataclasses import dataclass

from src.dqn.human_normalised import get_human_normalized_score
from src.dqn.utils import TimeScaleMeanBuffer
from src.dqn.agent import DQNAgent
from torchrl._utils import timeit

from typing import Any, Dict, NamedTuple

import numpy as np
import gymnasium as gym


class Experience(NamedTuple):
    """A single experience tuple from the environment."""

    observation: np.ndarray
    next_observation: np.ndarray
    action: int
    reward: float
    done: bool
    info: dict


class PeriodicTrigger:
    """Tracks and triggers events on periodic intervals."""

    def __init__(self, period: int):
        """Initialize the periodic trigger.

        Args:
            period: Number of steps between triggers
        """
        self.period = period
        self.last_trigger = -1  # Start at -1 so first check will trigger

    def should_trigger(self, current_step: int) -> bool:
        """Check if we should trigger based on current step.

        Args:
            current_step: Current step number

        Returns:
            bool: True if we should trigger
        """
        current_period = current_step // self.period
        if current_period > self.last_trigger:
            self.last_trigger = current_period
            return True
        return False

    @property
    def last_triggered_step(self) -> int:
        """Get the step number of the last trigger."""
        return self.last_trigger * self.period

    @property
    def current_period_start(self) -> int:
        """Get the start of the current period, rounded down to the nearest period.

        For example, if period=1M and we're at step 5.1M, this returns 5M.
        """
        return (self.last_trigger + 1) * self.period


@dataclass
class TrainerArgs:
    """Arguments for training setup and environment."""

    exp_name: str = os.path.basename(__file__)[: -len(".py")]
    """the name of this experiment"""
    seed: int = 1
    """seed of the experiment"""
    torch_deterministic: bool = True
    """if toggled, `torch.backends.cudnn.deterministic=False`"""
    cuda: bool = True
    """if toggled, cuda will be enabled by default"""
    capture_video: bool = False
    """whether to capture videos of the agent performances (check out `videos` folder)"""
    save_model: bool = False
    """whether to save model into the `runs/{run_name}` folder"""
    matmul_precision: str = "high"
    """the precision of matrix multiplication in pytorch"""
    progress_bar: bool = True
    """if toggled, show tqdm progress bar"""
    env_id: str = "ALE/Pong-v5"
    """the id of the environment"""
    continue_criteria_str: str = "train_for_total_decisions"
    """the criteria to continue training, either 'train_for_total_frames' or 'train_for_total_decisions'"""
    total_frames: int = 200_000_000
    """total frames of the experiments"""
    total_decisions: int = 40_000_000
    """total decisions of the experiments"""
    num_envs: int = 1
    """the number of parallel game environments"""
    frame_skip: int = 5
    """the number of frames to skip at each step"""


class GymTransition(NamedTuple):
    """Transition tuple generated by a gym environment."""

    observations: np.uint8
    actions: np.uint8
    next_observations: np.uint8
    rewards: np.float32
    terminated: np.bool
    truncated: np.bool
    info: Dict[str, Any]


class BatchTrainer:
    ARGS = TrainerArgs

    def __init__(
        self,
        args: TrainerArgs,
        fabric: Fabric,
        logger: WandbLogger,
        envs: gym.vector.SyncVectorEnv,
        agent: DQNAgent,
    ):
        """Initialize the batch trainer.

        Args:
            args: Combined training and agent arguments
            fabric: Pre-configured Fabric instance
            logger: Pre-configured WandbLogger instance
        """
        self.args: TrainerArgs = args
        self.fabric: Fabric = fabric
        self.logger: WandbLogger = logger

        assert args.num_envs == 1, "vectorized envs are not supported at the moment"

        # Setup directories
        self.data_path = logger.experiment.dir
        if args.save_model:
            os.makedirs(f"{self.data_path}/weights", exist_ok=True)
        if args.capture_video:
            os.makedirs(f"{self.data_path}/videos", exist_ok=True)

        # Seeding
        fabric.seed_everything(args.seed)
        torch.backends.cudnn.deterministic = args.torch_deterministic

        # Setup environment and agent
        self.envs = envs
        self.agent = agent

        # Initialize metrics
        self.start_time = time.time()
        self.step_count = 0
        self.e_count = 0
        self.frame_no = 0

        # Setup periodic triggers for different logging frequencies
        self.progress_trigger = PeriodicTrigger(10_000)  # Progress bar updates every 10k frames
        self.metrics_trigger = PeriodicTrigger(100_000)  # Log metrics every 100k frames
        self.checkpoint_trigger = PeriodicTrigger(25_000_000)  # Save model every 25M frames

        # Initialize metric buffers
        self.episode_reward = TimeScaleMeanBuffer(100)
        self.episodic_length = TimeScaleMeanBuffer(100)
        self.episode_time = TimeScaleMeanBuffer(100)
        self.per_episode_decision_count = 0
        self.per_episode_num_decisions_taken = TimeScaleMeanBuffer(100)
        self.per_episode_steps_per_decision = TimeScaleMeanBuffer(100)
        self.per_epsiode_frames_per_decision = TimeScaleMeanBuffer(100)
        self.reward_per_decision = TimeScaleMeanBuffer(100)
        self.reward_per_step = TimeScaleMeanBuffer(100)
        self.agent_keys = [
            "Replay Buffer Add",
            "Agent Action",
            "Replay Buffer Sample",
            "Update",
        ]
        self.environment_keys = ["Environment Step"]
        self.additional_metrics_to_log: Dict[str, Callable[[], Any]] = {}

    def execute_agent_decision(self, action) -> tuple[list[Experience], bool, bool, dict]:
        """Execute an action in the environment.

        Args:
            action: The action to execute

        Returns:
            Tuple of (next_obs, reward, termination, truncation, info)
        """
        with timeit("Environment Step"):
            next_obs, reward, termination, truncation, info = self.envs.step(action)
            self.reward_per_step.add(reward[0])

        self.step_count += 1
        self.frame_no = int(info["frame_number"][0])

        # Track normalized reward
        self.reward_per_decision.add(reward[0])

        experiences = [
            Experience(
                observation=self.obs,
                next_observation=next_obs,
                action=action[0],
                reward=reward[0],
                done=termination,
                info=info,
            )
        ]
        self.obs = next_obs
        return experiences, termination, truncation, info

    def _collect_episode_stats(self, info: Dict[str, Any]) -> None:
        """Collect and update episode statistics.

        Args:
            info: Environment info dictionary containing episode information
        """
        assert "episode" in info
        self.e_count += 1
        self.episode_reward.add(info["episode"]["r"][0])
        self.episodic_length.add(info["episode"]["l"][0])
        self.episode_time.add(info["episode"]["t"][0])
        if self.per_episode_decision_count > 0:
            self.per_episode_num_decisions_taken.add(self.per_episode_decision_count)
            self.per_episode_steps_per_decision.add(info["episode"]["l"][0] / self.per_episode_decision_count)
            self.per_epsiode_frames_per_decision.add(
                (info["episode"]["l"][0] * self.args.frame_skip) / self.per_episode_decision_count
            )
        self.per_episode_decision_count = 0

    def collect_rollouts(self) -> None:
        """Collect experience from the environment."""
        for _ in range(self.agent.args.train_frequency):
            with timeit("Agent Action"):
                epsilon = self.agent.get_exploration_rate(self.frame_no)
                if np.random.random() < epsilon:
                    action = self.agent.get_exploration_action()
                else:
                    with (
                        timeit("Agent Decision"),
                        torch.no_grad(),
                        self.fabric.autocast(),
                    ):
                        obs = torch.as_tensor(self.obs).to(self.fabric.device)
                        action = self.agent.policy(obs, self.agent.q_network)
                        self.agent.agent_decisions_made_so_far += 1
                        self.per_episode_decision_count += 1
                    action = action.cpu().numpy()

            experiences, termination, truncation, info = self.execute_agent_decision(action)

            with timeit("Replay Buffer Add"):
                for experience in experiences:
                    self.agent.store_experience(
                        obs=experience.observation,
                        next_obs=experience.next_observation,
                        action=experience.action,
                        reward=experience.reward,
                        done=experience.done,
                        info=experience.info,
                    )
                if "episode" in info:
                    self._collect_episode_stats(info)

    def prefill_buffer(self, pbar: Optional[tqdm] = None) -> None:
        """Prefill the replay buffer with random actions."""
        for i in range(self.agent.args.learning_starts):
            action = self.agent.get_exploration_action()

            experiences, termination, truncation, info = self.execute_agent_decision(action)

            with timeit("Replay Buffer Add"):
                for experience in experiences:
                    self.agent.store_experience(
                        obs=experience.observation,
                        next_obs=experience.next_observation,
                        action=experience.action,
                        reward=experience.reward,
                        done=experience.done,
                        info=experience.info,
                    )
                if "episode" in info:
                    self._collect_episode_stats(info)

            if pbar and self.args.progress_bar and i % 1000 == 0:
                if self.args.continue_criteria_str == "train_for_total_frames":
                    self.pbar.n = self.frame_no
                elif self.args.continue_criteria_str == "train_for_total_decisions":
                    self.pbar.n = self.agent.agent_decisions_made_so_far
                else:
                    raise ValueError(f"Unknown continue training criteria: {self.args.continue_criteria_str}")

                pbar.desc = f"{'frames': <8}: {self.frame_no} | {'agent steps': <8}: {self.step_count}"
                pbar.refresh()

    def train(self, continue_criteria_str="train_for_total_decisions") -> None:
        """Main training loop."""
        # Setup progress bar
        if self.args.progress_bar:
            if self.args.continue_criteria_str == "train_for_total_frames":
                total = self.args.total_frames
            elif self.args.continue_criteria_str == "train_for_total_decisions":
                total = self.args.total_decisions
            else:
                raise ValueError(f"Unknown continue training criteria: {self.args.continue_criteria_str}")

            self.pbar = tqdm(total=total)

        else:
            self.pbar = None

        self.obs, starting_info = self.envs.reset(seed=self.args.seed)
        self.frame_no = int(starting_info["frame_number"][0])
        del starting_info

        # Prefill replay buffer
        with timeit("Prefill Replay Buffer"):
            self.prefill_buffer(self.pbar)

        # Figure out how long to train based on continue criteria
        def train_for_total_frames() -> bool:
            """Check if training should continue based on total frames."""
            return self.frame_no < self.args.total_frames

        def train_for_total_decisions() -> bool:
            """Check if training should continue based on total decisions."""
            return self.agent.agent_decisions_made_so_far < self.args.total_decisions

        if continue_criteria_str == "train_for_total_frames":
            continue_criteria = train_for_total_frames
        elif continue_criteria_str == "train_for_total_decisions":
            continue_criteria = train_for_total_decisions
        else:
            raise ValueError(f"Unknown continue training criteria: {continue_criteria_str}")

        # Start training
        while continue_criteria():
            # Collect experience
            with timeit("Agent Rollout"):
                self.collect_rollouts()

            with timeit("Replay Buffer Sample"), self.fabric.autocast():
                data = self.agent.replay_buffer.sample(self.agent.args.batch_size)

            with timeit("Update"), self.fabric.autocast():
                loss, est_q_values = self.agent.update(data=data)
                loss = loss.item()
                est_q_values = est_q_values.mean().item()

            # Progress bar and logging
            if self.pbar and self.args.progress_bar and self.progress_trigger.should_trigger(self.frame_no):
                if continue_criteria_str == "train_for_total_frames":
                    self.pbar.n = self.frame_no
                elif continue_criteria_str == "train_for_total_decisions":
                    self.pbar.n = self.agent.agent_decisions_made_so_far
                else:
                    raise ValueError(f"Unknown continue training criteria: {continue_criteria_str}")

                self.pbar.desc = f"{'frames': <8}: {self.frame_no} | {'agent steps': <8}: {self.step_count} | {'agent decisions': <8}: {self.agent.agent_decisions_made_so_far} | {'agent updates': <8}: {self.agent.update_count}"
                self.pbar.refresh()

            if self.metrics_trigger.should_trigger(self.frame_no):
                with timeit("Logging"):
                    self.log_metrics(loss, est_q_values, self.frame_no)

            if self.args.save_model and self.checkpoint_trigger.should_trigger(self.frame_no):
                self.save_checkpoint(self.checkpoint_trigger.current_period_start)

        if self.pbar and self.args.progress_bar:
            if self.args.continue_criteria_str == "train_for_total_frames":
                self.pbar.n = self.frame_no
            elif self.args.continue_criteria_str == "train_for_total_decisions":
                self.pbar.n = self.agent.agent_decisions_made_so_far
            else:
                raise ValueError(f"Unknown continue training criteria: {self.args.continue_criteria_str}")

            self.pbar.desc = f"{'frames': <8}: {self.frame_no} | {'agent steps': <8}: {self.step_count} | {'agent decisions': <8}: {self.agent.agent_decisions_made_so_far} | {'agent updates': <8}: {self.agent.update_count}"
            self.pbar.refresh()

        with timeit("Logging"):
            data = self.agent.replay_buffer.sample(self.agent.args.batch_size)
            loss, est_q_values = self.agent.update(data=data)
            loss = loss.item()
            est_q_values = est_q_values.mean().item()
            self.log_metrics(loss, est_q_values, self.frame_no)

        self.envs.close()
        wandb.finish()

    def log_metrics(self, loss: float, est_q_values: float, stepped_frame: int) -> None:
        """Log metrics to the logger."""
        elapsed_time = time.time() - self.start_time
        timeit_percall = timeit.todict(prefix="Timeit_percall")
        timeit_accumulative = timeit.todict(percall=False, prefix="Timeit_acculumative")

        agent_time_percall = sum(timeit_percall["Timeit_percall/" + name] for name in self.agent_keys)
        environment_time_percall = sum(timeit_percall["Timeit_percall/" + name] for name in self.environment_keys)
        agent_time_accumulative = sum(timeit_accumulative["Timeit_acculumative/" + name] for name in self.agent_keys)
        environment_time_accumulative = sum(
            timeit_accumulative["Timeit_acculumative/" + name] for name in self.environment_keys
        )

        agent_environment_ratio = agent_time_percall / environment_time_percall
        agent_environment_ratio_accumulative = agent_time_accumulative / environment_time_accumulative

        per_epsiode_frames_per_decision = self.per_epsiode_frames_per_decision.mean()

        to_log = {
            "episode/episode_reward": self.episode_reward.mean(),
            "episode/human_normalized_score": get_human_normalized_score(self.args.env_id, self.episode_reward.mean()),
            "episode/episodic_length": self.episodic_length.mean(),
            "episode/episode_time": self.episode_time.mean(),
            "episode/num_decisions_taken": self.per_episode_num_decisions_taken.mean(),
            "episode/steps_per_decision": self.per_episode_steps_per_decision.mean(),
            "episode/frames_per_decision": per_epsiode_frames_per_decision,
            "episode/decisions_per_frame": 1 / self.per_epsiode_frames_per_decision.mean()
            if per_epsiode_frames_per_decision is not None
            else None,
            "agent/td_loss": loss,
            "agent/avg_q_values": est_q_values,
            "agent/rbuffer_size": self.agent.replay_buffer.size(),
            "agent/reward_per_decision_over_last_100_decisions": self.reward_per_decision.mean(),
            "agent/reward_per_step_over_last_100_steps": self.reward_per_step.mean(),
            "speed_per_sec/FPS": stepped_frame // elapsed_time,
            "speed_per_sec/SPS": self.step_count // elapsed_time,
            "speed_per_sec/UPS": self.agent.update_count // elapsed_time,
            "speed_per_frame/UPF": self.agent.update_count / stepped_frame,
            "speed_per_frame/SPF": self.step_count / stepped_frame,
            **timeit_percall,
            **timeit_accumulative,
            "summary_percall/agent_time": agent_time_percall,
            "summary_percall/environment_time": environment_time_percall,
            "summary_percall/agent_environment_ratio": agent_environment_ratio,
            "summary_accumulative/agent_time": agent_time_accumulative,
            "summary_accumulative/environment_time": environment_time_accumulative,
            "summary_accumulative/agent_environment_ratio": agent_environment_ratio_accumulative,
            "xaxis/env_frames": stepped_frame,
            "xaxis/env_steps": self.step_count,
            "xaxis/agent_decisions": self.agent.agent_decisions_made_so_far,
            "xaxis/agent_updates": self.agent.update_count,
            "xaxis/agent_target_update": self.agent.target_update_count,
            **{k: v() for k, v in self.additional_metrics_to_log.items()},
        }
        self.fabric.log_dict(to_log, step=stepped_frame)

    def save_checkpoint(self, stepped_frame: int) -> None:
        """Save a model checkpoint."""
        model_path = f"{self.data_path}/weights/{self.args.exp_name}-{stepped_frame}M.dqn_model"
        self.agent.save(model_path)
        print(f"model saved to {model_path}")
